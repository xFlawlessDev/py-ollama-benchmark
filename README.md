# Ollama Benchmark Suite Guide

## Overview

This repository contains a suite of tools designed to evaluate the performance and stability of Ollama models running on your local infrastructure. Whether you are testing Large Language Models (LLM) or Vision Language Models (VLM), these scripts help verify:
- **Inference Speed**: Tokens per second (T/s) for individual users.
- **System Capability**: Total system throughput under load.
- **Server Stability**: How the server handles concurrent requests (queuing vs parallel processing).

## Quick Start Table

| Script Name | Evaluation Target | Best For | Output |
| :--- | :--- | :--- | :--- |
| `benchmark-ollama.py` | Multi-Model Text | Comparing speed across different models (e.g., Qwen vs Llama). | Console Table + CSV |
| `benchmark-text-suite-ollama.py` | Text & Concurrency | Comprehensive test of Chat, Coding, and RAG pipelines under differing user loads. | Console + CSV |
| `benchmark-vision-suite-ollama.py` | Vision Models (VLM) | Testing image analysis capabilities (OCR, Description) and vision encoder latency. | Console + CSV |
| `server-load-test-ollama.py` | Stress Testing | Flooding the server to find breaking points or measuring maximum theoretical throughput. | Console Report Only |

---

## Detailed Usage Section

### 1. Basic Model Benchmark (`benchmark-ollama.py`)
**Purpose**: Runs a set of fixed scenarios (Creative Writing, Coding, Logic, JSON) against a list of models to compare raw inference speed.

**Command Line Usage**:
```bash
# Test specific models
python benchmark-ollama.py --models qwen3:30b deepseek-r1:32b

# Specify custom API URL
python benchmark-ollama.py --url http://192.168.1.100:11434/api/generate
```

### 2. Comprehensive Text Suite (`benchmark-text-suite-ollama.py`)
**Purpose**: Simulates real-world application usage patterns (Chat, Heavy Coding, RAG) with increasing user concurrency (1, 8, 16, 32 users).

**Command Line Usage**:
```bash
# Default (uses qwen3:30b and qwen3-embedding:4b)
python benchmark-text-suite-ollama.py

# Custom models
python benchmark-text-suite-ollama.py -m llama3.1:70b --embed-model nomic-embed-text
```

### 3. Comprehensive Vision Suite (`benchmark-vision-suite-ollama.py`)
**Purpose**: Tests Vision Language Models (like Qwen-VL, Llama-Vision) by sending images for analysis. Measures how long the "Vision Encoder" takes versus text generation.

**Command Line Usage**:
```bash
# Default (uses qwen3-vl:30b)
python benchmark-vision-suite-ollama.py

# Custom model
python benchmark-vision-suite-ollama.py -m llama3.2-vision
```

### 4. Server Load Tester (`server-load-test-ollama.py`)
**Purpose**: A stress-test tool. It sends `N` requests exactly simultaneously to see how the server queues or processes them.

**Command Line Usage**:
```bash
# Simulate 10 concurrent users (Default)
python server-load-test-ollama.py

# Simulate 50 concurrent users on a specific model
python server-load-test-ollama.py -u 50 -m qwen2.5:14b
```

---

## Key Metrics Explained

| Metric | Definition | Why it matters |
| :--- | :--- | :--- |
| **TPS (Tokens/Sec)** | Speed of token generation for a *single* user. | How fast the user feels the text appearing. |
| **Latency** | Total time from sending a request to finishing the response. | The wait time experienced by the user. |
| **Prompt Eval Duration** | Time taken to process the input (text or image) before generating text. | Critical for RAG (long context) or Vision (image processing) apps. |
| **System Throughput** | Total tokens generated by the server per second across *all* active users. | Measures the raw horsepower of your GPU/Server. |

---

## Analyzing Results

### Reading Console Output
- **Pass/Fail**: In the suite scripts, "Fail" usually means a timeout (server took too long) or an HTTP 500 error.
- **WARN**: If failures occur but are below 20% of requests.
- **CRITICAL**: If 100% of requests fail.

### CSV Logs
Results are saved to the `Result/` folder or root directory with timestamps (e.g., `text_benchmark_20260114_1723.csv`).
**Important Columns**:
- `users`: Number of concurrent requests.
- `avg_tps`: Average speed per user.
- `sys_throughput`: Verification of parallel scaling.

### ⚠️ Important Note on Concurrency
If you run a test with **10 Users**, but your `avg_tps` drops significantly compared to 1 User, or if the `latency` increases linearly (e.g., 1 user = 5s, 10 users = 50s), this indicates **Queuing**.

This happens when the Ollama server is configured to handle fewer parallel requests than you are testing.
*   **Sequential Processing**: The server handles User 1, then User 2, etc. (High Latency, High queues).
*   **Parallel Processing**: The server handles User 1 and User 2 simultaneously (Speed splits between them, but Latency stays low).

To fix this, check your Ollama server variables:
*   `OLLAMA_NUM_PARALLEL`: Controls how many requests can run at once.
*   `OLLAMA_MAX_LOADED_MODELS`: Controls how many models can be in VRAM.

## Requirements

*   Python 3.8+
*   `requests` library

```bash
pip install requests